# Quiz Chatbot IA ü§ñ
---
Ce projet Python + React a √©t√© r√©alis√© dans le cadre du devoir pour comparer les embeddings de questions/r√©ponses avec le QCM de l'√©valuation du module.

## üéì D√©marche 

Le sujet du devoir portait sur la comparaison d'embeddings mais nous avons souhait√© construire une application avec trois modes compl√©mentaires :

1. **üí¨ Chat libre**  
   Une interaction directe avec le LLM. L‚Äôutilisateur peut poser une question libre, et observer une r√©ponse g√©n√©r√©e automatiquement (sans contexte documentaire).

![Aper√ßu du projet](./frontend/src/assets/capture.png)


2. **üß™ QCM intelligent**  
   L'utilisateur lance une √©valuation automatique de toutes les questions du QCM.  
   Chaque question est :
  - pos√©e au LLM via un prompt g√©n√©rique,
  - compar√©e aux 4 choix du QCM √† l‚Äôaide d‚Äô**embeddings**,
  - √©valu√©e automatiquement (bonne r√©ponse, pr√©diction, similarit√© cosinus, etc.).

   > üß† Ce mode met en ≈ìuvre la **comparaison d‚Äôembeddings** avec `sentence-transformers` et `sklearn.metrics.pairwise.cosine_similarity`.

Aper√ßu de la r√©ponse pour la comparaison d'embeddings

![Aper√ßu de la comparaison](./frontend/src/assets/capture2.png)

3. **üìö Mode Cours (RAG)**  
   Ce mode applique la strat√©gie RAG (Retrieval-Augmented Generation) :
  - Indexation du cours (`txt` ou `pdf`) mais nous avons retenu les pdf car la transformation en txt n'√©tait pas concluante,
  - Interrogation du mod√®le via `langchain` + `Chroma`
  - Il indique en r√©ponse les documents et les 'chunks' c'est √† dire les morceaux de donn√©es sur lesquels il s'est appuy√©
  
![Aper√ßu du projet](./frontend/src/assets/capture1.png)

---

## üìÅ Structure du projet
### Backend

- `backend/qcm/` : traitement du QCM, comparaison d'embeddings, g√©n√©ration de r√©sultats pour un futur fine-tuning
- `backend/rag/` : traitement des documents pour la recherche augment√©e (RAG)
- `ask_free.py` : chat libre (invoque le mod√®le sans contexte)

### Frontend
- `api.js` : routes API (chat, qcm, rag)
- `src/components/` :
  - `ChatTab.jsx` : interface du chat libre avec animation de r√©ponse
  - `QcmSelector.jsx` : chargement et affichage des questions, d√©clenchement de l‚Äô√©valuation
  - `QcmResultModal.jsx` : affichage clair des similarit√©s et des r√©ponses
  - `RagChat.jsx` : interface du mode cours (RAG)
- `App.jsx` : int√©gration des trois modes via onglets et gestion des √©tats globaux
---

## üöÄ Lancer le projet

### Ollama
Il est n√©cessaire de d√©marrer localement ollama et le mod√®le gemma:2b avec la commande :
```bash
ollama run gemma:2b
````

### Backend
```bash
cd backend
# Installer les d√©pendances n√©cessaires
poetry install
````

--- 

### üñ•Ô∏è Frontend
```bash
cd frontend
npm install
npm run dev       # lance l‚Äôinterface React avec Vite
````

### üåê API ‚Äì Description des routes

| Route                     | M√©thode | Description                                                                                                                                                       |
|--------------------------|---------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `/api/qcm/evaluate-qcm`  | POST    | √âvalue tout le QCM (`eval.csv`). Pour chaque question, l'IA g√©n√®re une r√©ponse, calcule les **embeddings**, compare avec les 4 choix et retourne : <br>‚Ä¢ la r√©ponse du mod√®le <br>‚Ä¢ l‚Äôindex pr√©dit <br>‚Ä¢ l‚Äôindex correct <br>‚Ä¢ les similarit√©s cosinus |
| `/ask-free`              | POST    | Pose une **question libre** √† l‚ÄôIA (mode chat sans contexte). Retourne une r√©ponse simple du LLM (`gemma:2b`).                                                    |
| `/ask-rag`               | POST    | Mode **RAG** (*Retrieval-Augmented Generation*) : l‚ÄôIA s‚Äôappuie sur les documents vectoris√©s du dossier `documents/cours/` pour formuler sa r√©ponse.            |
| `/ask-one` *(optionnel)* | POST    | √âvalue **une seule question** (envoy√©e dans le corps de la requ√™te). Utile pour tester le comportement du mod√®le sur une question isol√©e.                        |

> ‚úÖ Les routes utilis√©es en production sont principalement :  
> `/api/qcm/evaluate-qcm`, `/ask-free`, et `/ask-rag`.

---



## üôè Remerciements

Nous tenons √† vous remercier pour votre attention.

Bien que le sujet initial ait uniquement demand√© une comparaison d'embeddings, nous avons choisi d‚Äô√©largir la port√©e du projet pour apprendre davantage sur l‚Äôint√©gration front-back, les cha√Ænes RAG, et les interactions LLM c√¥t√© client.

Nous n'avons finalement pas proc√©d√© au fine-tuning du mod√®le car cela nous paraissait trop complexe √† impl√©menter dans notre contexte local (ressources, gestion des checkpoints, dur√©e, etc.), mais nous avons g√©n√©r√© des r√©sultats  dans ce but (fichier CSV de sortie avec pr√©dictions).

Ce projet nous a permis de consolider nos comp√©tences et de mieux comprendre les limites et usages r√©els des LLM.

Merci pour votre compr√©hension et votre temps !