Question,Réponse 1,Réponse 2,Réponse 3,Réponse 4,Bonne réponse
En quoi consiste la phase de fine-tuning d'un LLM ?,La phase de fine-tuning d'un LLM consiste à ajuster les paramètres initialisés aléatoirement pour lui permettre de compléter une phrase.,La phase de fine-tuning d'un LLM consiste à ajuster les paramètres pour lui permettre de transformer une phrase en un vecteur.,La phase de fine-tuning d'un LLM consiste à ajuster les paramètres pré-appris d'un modèle pré-entraîné sur une tâche spécifique de NLP.,La phase de fine-tuning d'un LLM consiste à nettoyer la question posée au modèle pour retirer les mots et les conjugaisons inutiles.,3
"Dans le contexte du NLP, qu'est-ce que l'analyse de sentiment ?","Dans le contexte du NLP, l'analyse de sentiment permet de résumer un texte donné en paramètre","Dans le contexte du NLP, l'analyse de sentiment est la tâche de trouver le thème principal d'un texte donné.","Dans le contexte du NLP, l'analyse de sentiment consiste à répondre à une question sans accès à des sources d'informations","Dans le contexte du NLP, l'analyse de sentiment consiste à classer le sentiment associé à un texte donné en différentes catégories telles que positif, négatif ou neutre.",4
"Dans un texte analysé avec l'algorithme TF-IDF, quel mot aura le plus grand score ?",Le mot qui aura le plus grand score TF-IDF dans un texte sera celui qui apparaît fréquemment dans ce texte (score TF élevé) tout en étant rare dans l'ensemble du corpus (score IDF élevé).,Le mot qui aura le plus grand score TF-IDF dans un texte sera celui qui apparaît fréquemment dans ce texte (score TF élevé) tout en apparaissant fréquemment dans l'ensemble du corpus (score IDF bas).,Le mot qui aura le plus grand score TF-IDF dans un texte sera celui qui apparaît le plus rarement dans ce texte (score TF bas) tout en apparaissant fréquemment dans l'ensemble du corpus (score IDF bas).,Le mot qui aura le plus grand score TF-IDF dans un texte sera celui qui apparaît le plus rarement dans ce texte (score TF bas) tout en étant rare dans l'ensemble du corpus (score IDF élevé).,1
Quel est l'obstacle majeur lorsque l'on demande à un LLM de résumer un texte ?,Les LLMs ne sont pas capable de trouver les mots les plus importants dans un texte.,"La réponse d'un LLM contient le même nombre de tokens que le nombre de tokens présents dans la question posée, il ne pourra donc pas résumer le texte","Il n'y a aucun obstacle à résumer un texte avec un LLM, quelle que soit la taille de ce texte.",Les LLMs ont des contextes limités en nombre de tokens. La plupart des modèles ne résume pas correctement un texte de plus de 4000 tokens.,4
Pourquoi le NLP est-il difficile lorsqu'on l'applique à une langue rare ?,"Le NLP est difficile à appliquer à une langue rare, car un LLM ne parle qu'une seule langue, il faut donc réentraîner le modèle uniquement sur des textes de la langue utilisée.",Le NLP peut être particulièrement difficile lorsqu'il est appliqué à une langue rare en raison du manque de données et des pré-traitements appliqués inadaptés.,Le NLP appliqué à une langue rare est très difficile car les LLMs ne comprennent que les caractères ASCII.,Le NLP appliqué à une langue rare est très difficile car la tokenisation d'un mot est impossible si ce mot n'est pas présent dans le vocabulaire.,2
Pourquoi les LLMs ont-ils tendance à se répéter ?,Les LLMs ont tendance à se répéter car leur vocabulaire est très limité.,"Les LLMs ont tendance à se répéter car ils génèrent le texte au fur et à mesure, sans avoir connaissance des mots qu'ils ont déjà générés précédemment.",Les LLMs prédisent la probabilité d'apparition du token suivant. Les mots les plus utilisés ont de fortes chances de réapparaître plusieurs fois.,"Les LLMs ont des contextes très limités d'une dizaine de tokens, ils n'ont donc connaissance que des quelques mots précédent.",3
Que fait la couche Dropout dans les LLMs ?,"Au cours de l'entraînement, la couche Dropout réinitialise les paramètres de certains neurones du LLM pour éviter le sur-entraînement et la coadaptation.","La couche Dropout supprime aléatoirement certains neurones du LLM au cours de l'entraînement, et les recrée à d'autres endroits du réseau.","La couche Dropout désactive aléatoirement un pourcentage fixe de neurones pendant l'entraînement. Cela réduit la coadaptation, évite le surapprentissage, et améliore la généralisation.","Au cours du fine-tuning, la couche Dropout génère aléatoirement des résultats aberrants, pour éviter le surapprentissage, réduire la coadaptation et améliorer la généralisation.",3
Quel algorithme peut-on utiliser pour détecter 2 phrases similaires dans leur signification ?,L'algorithme de distance cosinus permet de mesurer la similarité entre les deux embedding de phrases en calculant le cosinus de l'angle entre eux. ,La métrique ROUGE peut être utilisée pour savoir si 2 phrases ont le même sens malgré l'utilisation de mots différents.,"Une fois que les 2 phrases ont été transformées en vecteur avec Word2Vec, plus la distance euclidienne entre les 2 vecteurs est grande, plus leurs significations sont proches.",On peut mesurer la similarité entre 2 phrases en comptant le nombre de stop words communs.,1
Doit-on utiliser un modèle de word embedding pour alimenter le fine-tuning d'un LLM ?,"Oui, avant de soumettre un prompt à la fonction generate() d'un modèle PreTrainedModel, il faut utiliser un modèle d'embedding sémantique comme Word2Vec pour transformer le texte en un vecteur.","Non, il faut soumettre le prompt sous forme textuelle directement au modèle.","Oui, le prompt doit être transformé en vecteur grâce à un LLM d'embedding sémantique fourni à côté du LLM que l'on souhaite fine-tuner.","Non, seul un tokenizer est utilisé en amont d'un modèle LLM, et ce tokenizer doit être le même que celui utilisé au moment de l'entraînement du LLM.",4
Quel est le problème d'utiliser word2vec pour réaliser l'embedding d'une phrase complète ?,"Le problème de Word2Vec est qu'il a besoin du contexte de la phrase, le texte où elle a été trouvée, pour réaliser son embedding.","Word2Vec calcule un embedding pour chaque mot indépendamment de son contexte, ce qui peut entraîner une perte d'information sur le sens global de la phrase.","Le problème de Word2Vec est qu'il ne peut pas réaliser l'embedding des mots de la phrase qui sont conjugués, ou accordés.","Le problème est que pour faire calculer l'embedding d'une phrase complète par Word2Vec, il faut lui donner chaque mot de la phrase dans l'ordre, un par un, en lui donnant comme contexte les mots précédents.",2
L'analyse PCA permet-elle une représentation fiable de l'embedding de plusieurs phrases ?,L'analyse PCA est la meilleure méthode pour visualiser et comparer les embedding de plusieurs phrases.,"L'analyse PCA n'est pas fiable pour comparer des embedding, car elle va minimiser les distances entre des phrases très éloignées sémantiquement.","L'analyse PCA ne conserve pas les distances phrase à phrase, et tend plutôt à maximiser la variance des points. En cela, elle est moins fiable qu'une analyse t-SNE ou UMAP.","L'analyse PCA est fiable pour comparer des embedding, car elle va minimiser les distances entre des phrases très proches sémantiquement.",3
Les LLMs peuvent-ils prendre en compte des caractères spéciaux ?,"Non les LLMs ne prennent pas en compte les caractères spéciaux, seuls les caractères ASCII sont conservés dans les corpus d'entraînement, les autres caractères sont supprimés","Les LLMs remplacent systématiquement les caractères spéciaux par leur équivalent non accentué, ou une combinaison de caractères équivalente.","Les LLMs prennent en compte tous les caractères spéciaux, et sont capable de reconnaître tous les mots qui ont pu apparaître dans leurs corpus d'entraînement, par exemple, ""œil"" est toujours considéré comme un seul token.","Les LLMs ne peuvent prendre en compte les caractères spéciaux que si ils étaient suffisamment présents dans leurs corpus d'entraînements pour faire partie de leur vocabulaire, sinon le token est découpé caractère par caractère.",4
La tokenisation d'un mot par un LLM dépend-elle de la casse et la ponctuation associée ?,"Non, la tokenisation d'un mot par un LLM sera toujours la même quelle que soit sa place dans la phrase, la ponctuation qui l'entoure,et la présence de caractères en majuscule.","La tokenisation d'un mot par un LLM ne dépend pas de la ponctuation qui l'entoure, mais elle varie en fonction de la présence de caractères en majuscule.",Les LLMs prennent en compte à la fois la casse et la ponctuation dans leur tokenisation.,"La tokenisation d'un mot par un LLM varie en fonction de la ponctuation qui l'entoure, mais elle ne dépend pas de la présence de caractères en majuscule.",3
Est-il possible de faire tourner GPT-3 sur son ordinateur personnel ?,"Non, il n'est pas possible de faire fonctionner GPT-3 sur un ordinateur personnel, cela nécessite des cartes graphiques avec de grandes quantités de VRAM.","Oui, car j'ai un ordinateur personnel avec une carte graphique Titan.","Oui, à condition que l'ordinateur ait une carte graphique avec plus de 16Go de VRAM.","Oui, ChatGPT peut tourner sur un CPU.",1
La métrique d'évaluation ROUGE prend-elle en compte le sens des phrases qu'elle compare ?,"Oui, la métrique d'évaluation ROUGE compare le sens des phrases, ""Je roule en vélo"" aura le même sens que ""Je roule en bicyclette"".","Non, ROUGE évalue la correspondance des mots entre la référence et ce qui est prédit, sans prendre en compte la sémantique des phrases.","Non, ROUGE ne compare que le nombre de mots générés et le nombre de mots attendus.","Non, ROUGE fait une comparaison stricte entre le texte attendu et le texte prédit, les 2 textes doivent être exactement les mêmes, sinon la prédiction est considérée comme fausse.",2
Qu'est-ce qu'un prompt zéro-shot ?,Le terme zéro-shot indique qu'aucun exemple d'entraînement pour cette tâche spécifique n'a été fourni en contexte de la question.,Le terme zéro-shot indique que le LLM n'a jamais été entraîné à répondre à ce type de questions.,Le terme zéro-shot indique que le prompt envoyé au LLM contient la réponse à la question posée.,Le terme zéro-shot indique qu'il n'est pas possible de répondre à la question posée.,1
L'outil de déploiement de LLM Ollama peut-il fonctionner sans carte graphique ?,"Non, Ollama vérifie la présence d'une carte graphique au démarrage.","Ollama pourra démarrer même si il n'y a pas de carte graphique, mais il ne pourra pas charger le LLM dans la RAM.","Oui, car Ollama est conçu pour réduire la précision des LLMs afin de ne pas utiliser la carte graphique.","Ollama pourra démarrer même si il n'y a pas de carte graphique, mais si le LLM à déployer nécessite trop de ressources, cela sera très lent.",4
Qu'est-ce qu'une base de données comme Weaviate peut apporter à un système de RAG ?,Weaviate va permettre de retourner la réponse à une question à condition que le couple question-réponse a été indexé dans la base.,Weaviate va permettre d'entraîner le LLM du système RAG à partir des couples questions-réponses qui y auront été stockés. ,"Weaviate va permettre de stocker de la documentation avec leur représentation vectorielle, fournissant des fonctionnalités de recherche sémantique qui alimenteront le prompt du système RAG.",Weaviate permet de mettre en cache les réponses du LLM pour réduire drastiquement le temps de réponse du logiciel.,3
A quoi sert le framework langchain ?,"Le framework Langchain permet de déployer un LLM sans consommer trop de ressources, pour pouvoir l'utiliser sur un serveur de production à moindre coût.",Langchain est conçu pour faciliter le développement de logiciels basés sur le NLP. Il permet de créer des pipelines de traitement en enchaînant différentes étapes de transformation.,"Le framework Langchain permet de traiter de gros volumes de données en parallèle, en les distribuant sur plusieurs nœuds d'un cluster.",Le framework Langchain permet de concevoir les différentes couches d'un réseau de neurones pour créer un LLM adapté au besoin métier.,2
Est-ce qu'il est utile de développer des chatbots ?,"Oui, les chatbots peuvent fournir une expérience utilisateur personnalisée et engageante, en répondant aux questions des utilisateurs et en offrant une assistance instantanée.","Non, il n'est pas utile de développer un chatbot, cela coûte cher et ce n'est pas écologique !","Non, il n'est pas utile de développer un chatbot, il vaut mieux avoir de vraies personnes derrière un téléphone !","Non, il n'est pas utile de développer un chatbot, cela ne donne pas de bons résultats !",1
